---
layout: post
title: 핸즈온_머신러닝(2)
subtitle : 머신러닝 프로젝트 처음부터 끝까지
tags: [머신러닝]
author: sunsoo Kim
comments : False
---

# Chaper 02 - 머신러닝 프로젝트 처음부터 끝까지

## 2.1 실제 데이터로 작업하기
StatLib 저장소에 있는 캘리포니아 주택 가격(California Housing Prices) 데이터셋을 사용하도록 하겠습니다. 이 데이터셋은 1990년 캘리포니아 인구조사 데이터를 기반으로 만들어진 데이터를 기반으로 하고 있습니다.

## 2.2 큰 그림 보기
이 데이터는 캘리포니아의 블록 그룹(block group)마다 인구(population), 중간 소득(median income), 중간 주택 가격(median housing price)등을 담고 있습니다.

### 2.2.1 문제 정의
이 인구 조사 데이터로 어떤 비즈니스를 할수 있을까요? 이번에는 주택 가격을 예측하는 모델을 훈련시켜보려 합니다. 그리고 그에 맞추어서 전처리와 모델을 선택 해가는 방향으로 해보겠습니다.

### 2.2.2 성능 측정 지표 선택
주택 가격은 연속형 데이터로, 회귀 에 적합합니다. 그리고 회귀 문제의 대표적인 성능 지표는 평균 제곱근 오차(RMSE, Root Mean Square Error)로 공식은 다음과 같습니다.

$$
\text{RMSE}(\mathbf{X}, h) = \sqrt{\frac{1}{m} \sum^{m}_{i=1}{\left( h \left(\mathbf{x}^{(i)} \right) - y^{(i)} \right)^{2}}}
$$



```python

from __future__ import division, print_function, unicode_literals

# 공통
import numpy as np
import os

# 일관된 출력을 위해 유사난수 초기화
np.random.seed(56)

# 맷플롯립 설정
%matplotlib inline:
import matplotlib
import matplotlib.pyplot as plt
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12

# 한글출력
matplotlib.rc('font', family='NanumBarunGothic')
plt.rcParams['axes.unicode_minus'] = False

# 그림을 저장할 폴드
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "end_to_end_project"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)
```

## 2.3 데이터 파악

### 2.3.1 데이터 받아오기

 데이터를 다운받아 봅시다. csv를 다운 받을 수 있지만, **코드를 통해** url을 통해 tgz 파일을 내려 받고, 압축을 풀어보도록 하겠습니다. 
 
 코드는 다음과 같습니다. 
 - 우선 DOWN_ROOT, HOUSING_PATH, HOUSING_URL에 다운받을 url과 경로를 지정해줍니다. 
 - 이후 fatch_housing_data() 함수를 만듭니다. 
     
데이터 출처는 다음과 같습니다.
 - 출처: [California Housing Prices](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html)


```python
import os
import tarfile
from six.moves import urllib

DOWNLOAD_ROOT = "https://raw.githubusercontent.com/ageron/handson-ml/master/"
HOUSING_PATH = os.path.join("datasets", "housing")
HOUSING_URL = DOWNLOAD_ROOT + "datasets/housing/housing.tgz"

def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):
    if not os.path.isdir(housing_path):
        os.makedirs(housing_path)
    tgz_path = os.path.join(housing_path, "housing.tgz")
    urllib.request.urlretrieve(housing_url, tgz_path)
    housing_tgz = tarfile.open(tgz_path)
    housing_tgz.extractall(path=housing_path)
    housing_tgz.close()
```

fetch_housing_data() 함수를 실행해줍니다.


```python
fetch_housing_data()
```

이제 판다스를 사용해, 데이터를 읽어보도록 하겠습니다. load_housing_data() 함수를 만들어, 다음과 같이 csv를 출력하도록 해줍시다. 
그리고 나서 출력된 결과를 housing()라고 지정 해주도록 합시다. 앞으로 housing를 가지고 사용하도록 할 예정입니다.


```python
import pandas as pd

def load_housing_data(housing_path=HOUSING_PATH):
    csv_path = os.path.join(housing_path, "housing.csv")
    return pd.read_csv(csv_path)

housing = load_housing_data()
```

### 2.3.2 데이터 구조 파악 하기

#### a) 상위 5개 데이터 파악

우선 상위 5개의 데이터를 파악해 봅시다. dataframe()의 head()메서드를 사용합니다.


```python
housing.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>median_house_value</th>
      <th>ocean_proximity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-122.23</td>
      <td>37.88</td>
      <td>41.0</td>
      <td>880.0</td>
      <td>129.0</td>
      <td>322.0</td>
      <td>126.0</td>
      <td>8.3252</td>
      <td>452600.0</td>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-122.22</td>
      <td>37.86</td>
      <td>21.0</td>
      <td>7099.0</td>
      <td>1106.0</td>
      <td>2401.0</td>
      <td>1138.0</td>
      <td>8.3014</td>
      <td>358500.0</td>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-122.24</td>
      <td>37.85</td>
      <td>52.0</td>
      <td>1467.0</td>
      <td>190.0</td>
      <td>496.0</td>
      <td>177.0</td>
      <td>7.2574</td>
      <td>352100.0</td>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-122.25</td>
      <td>37.85</td>
      <td>52.0</td>
      <td>1274.0</td>
      <td>235.0</td>
      <td>558.0</td>
      <td>219.0</td>
      <td>5.6431</td>
      <td>341300.0</td>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-122.25</td>
      <td>37.85</td>
      <td>52.0</td>
      <td>1627.0</td>
      <td>280.0</td>
      <td>565.0</td>
      <td>259.0</td>
      <td>3.8462</td>
      <td>342200.0</td>
      <td>NEAR BAY</td>
    </tr>
  </tbody>
</table>
</div>



#### b) 컬럼들에 대한 구조적 특성 파악

이제 info()메서드를 사용해 데이터에 대한 간략한 설명과, 데이터 개수, 데이터 타입을 확인해 봅시다. 이를 통해 다음을 알 수가 있습니다. 
- 데이터의 행수는 20640개이다.
- 데이터의 컬럼수는 10개이다.
- total_bedrooms에는 20433이 non-null이므로 207개 값이 Null(결측치)입니다.
- ocean_proximity를 제외한 나머지는 모두 숫자 데이터


```python
housing.info()
```

    <class 'pandas.core.frame.DataFrame'>
    RangeIndex: 20640 entries, 0 to 20639
    Data columns (total 10 columns):
    longitude             20640 non-null float64
    latitude              20640 non-null float64
    housing_median_age    20640 non-null float64
    total_rooms           20640 non-null float64
    total_bedrooms        20433 non-null float64
    population            20640 non-null float64
    households            20640 non-null float64
    median_income         20640 non-null float64
    median_house_value    20640 non-null float64
    ocean_proximity       20640 non-null object
    dtypes: float64(9), object(1)
    memory usage: 1.6+ MB


##### b-1) ocean_proximity 데이터별 개수 파악

head(), info()메서드로 확인해본 결과, ocean_proximity데이터가 숫자형이 아니면서, 반복되는 걸 볼 수가 있습니다. 그렇다면 범주형데이터임을 조심럽게 추측해볼수 있는데, value_counts()메서드를 통해 확인해 보도록 합시다.<br>
1H OCEAN = 9136개, INLAND = 6551개, NEAR OCEAN = 2658개, NEAR BAY = 2290개, ISLAND = 5개로 범주형 데이터로 확인할수 있습니다.


```python
housing["ocean_proximity"].value_counts()
```




    <1H OCEAN     9136
    INLAND        6551
    NEAR OCEAN    2658
    NEAR BAY      2290
    ISLAND           5
    Name: ocean_proximity, dtype: int64



#### c) 숫자형 데이터의 기본적인 통계정보 파악

이번엔 describe()메서드를 사용해 숫자형 데이터의 기본적인 통계정보를 확인해봅시다. 
- count: Null이 아닌 값들의 개수를 알 수 있습니다. 
- mean: 평균을 알 수 있습니다. 
- std: 표준편차를 알 수 있습니다.
- min: 최소값을 알 수 있습니다.
- 25%: 1분위수를 알 수 있습니다. 데이터의 1/4지점에 해당하는 값입니다. 
- 50%: 중간값이라고 배워온 수치 입니다. 데이터의 정확히 중간에 위치한 값입니다.
- 75%: 3분위수라고 합니다. 데이터의 3/4지점에 해당하는 값입니다.
- max: 최대값을 알 수 있습니다.


```python
housing.describe()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>median_house_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20433.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
      <td>20640.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>-119.569704</td>
      <td>35.631861</td>
      <td>28.639486</td>
      <td>2635.763081</td>
      <td>537.870553</td>
      <td>1425.476744</td>
      <td>499.539680</td>
      <td>3.870671</td>
      <td>206855.816909</td>
    </tr>
    <tr>
      <th>std</th>
      <td>2.003532</td>
      <td>2.135952</td>
      <td>12.585558</td>
      <td>2181.615252</td>
      <td>421.385070</td>
      <td>1132.462122</td>
      <td>382.329753</td>
      <td>1.899822</td>
      <td>115395.615874</td>
    </tr>
    <tr>
      <th>min</th>
      <td>-124.350000</td>
      <td>32.540000</td>
      <td>1.000000</td>
      <td>2.000000</td>
      <td>1.000000</td>
      <td>3.000000</td>
      <td>1.000000</td>
      <td>0.499900</td>
      <td>14999.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>-121.800000</td>
      <td>33.930000</td>
      <td>18.000000</td>
      <td>1447.750000</td>
      <td>296.000000</td>
      <td>787.000000</td>
      <td>280.000000</td>
      <td>2.563400</td>
      <td>119600.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>-118.490000</td>
      <td>34.260000</td>
      <td>29.000000</td>
      <td>2127.000000</td>
      <td>435.000000</td>
      <td>1166.000000</td>
      <td>409.000000</td>
      <td>3.534800</td>
      <td>179700.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>-118.010000</td>
      <td>37.710000</td>
      <td>37.000000</td>
      <td>3148.000000</td>
      <td>647.000000</td>
      <td>1725.000000</td>
      <td>605.000000</td>
      <td>4.743250</td>
      <td>264725.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>-114.310000</td>
      <td>41.950000</td>
      <td>52.000000</td>
      <td>39320.000000</td>
      <td>6445.000000</td>
      <td>35682.000000</td>
      <td>6082.000000</td>
      <td>15.000100</td>
      <td>500001.000000</td>
    </tr>
  </tbody>
</table>
</div>



###  2.3.3 히스토그램

이번엔 시각화를 통해 데이터의 파악을 조금 더 직관적으로 해보도록 합시다. 히스토그램을 그려볼건데, 다음의 조건이 필요 합니다. hist() 메서드를 사용하기 위해서는 matplotlib을 사용해야 하고 이를 위해서는 `%matplotlib inline`코드를 사용해 미리 설정을 해주어야만 합니다. 그리고 나서 다음의 코드를 실행해보면 숫자데이터 9개 각각의 히스토그램을 알 수 있습니다.


```python
%matplotlib inline
import matplotlib.pyplot as plt
housing.hist(bins=50, figsize=(20,15))
plt.show()
```


![png](https://user-images.githubusercontent.com/45762604/83213805-08a29980-a19e-11ea-9076-f7aeec695e1a.png)


## 2.3.4 테스트 데이터 생성

이제 이 데이터를 Train/Test데이터로 분리를 해보도록 합시다. Train/Test를 분리 해주는 이유는 간단합니다. 전체 데이터의 일부분을 떼어, 평가를 해주어 모델의 과적합을 방지하기 위함인데요. 이번에는 20%정도르 Test set을 만들어 보도록 하겠습니다.

### a) np.random.permutaion()

np.random.permutaion 을 사용한 Train/Test 분리 코드 입니다.


```python
np.random.seed(56)
import numpy as np

def split_train_test(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]
```


```python
train_set, test_set = split_train_test(housing, 0.2)
print(len(train_set), "train +", len(test_set), "test")
```

    16512 train + 4128 test


### b) zlib의 crc32를 이용한 Train/Test set 나누기

위의 코드로는 데이터셋을 나누는데 있어 매번 달라질 수 있다는 단점이 있습니다. 물론 np.random.seed()함수를 사용해 초기값을 설정해 줄수 있지만 이를 해결할 수 있는 코드는 다음과 같습니다.


```python
from zlib import crc32

def test_set_check(identifier, test_ratio):
    return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32

def split_train_test_by_id(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]
```


```python
housing_with_id = housing.reset_index()  
train_set, test_set = split_train_test_by_id(housing_with_id, 0.2, 'index')
print(len(train_set), "train +", len(test_set), "test")
```

    16512 train + 4128 test


### c) sklearn의 train_test_split을 이용한 Train/Test set 나누기

이번에는 사이킷런을 이용해서 데이터셋을 나눠보도록 하겠습니다. 사이킷런의 train_test_split()함수를 사용하려고 합니다. 이는 두가지 특징이 있는데, 다음과 같습니다.
- 난수 초깃값을 설정해주는 random_state옵션이 있습니다.
- 행의 개수가 같은 여러 개의 데이터셋을 넘겨서 같은 ㅇ니덱스를 기반으로 나눌 수 있습니다.


```python
from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=56)
print(len(train_set), "train +", len(test_set), "test")
```

    16512 train + 4128 test


### d) sklearn의 StratifiedShuffleSplit을 이용한 Train/Test set 나누기

이번에는 계층적 샘플링(Stratified sampling)을 통해 데이터를 분리 해보도록 하겠습니다. 계층적 샘플링의 기본개념을 간단히 설명하자면 다음과 같이 설명할 수 있습니다.<br><br>
1년에 비오는 날이 약 20% 비가 오지 않는 날이 80%라고 합니다. 365일중에 약 73일이 비가 오는 날이 되겠고 나머지가 비가 오지 않는 날이 되겠는데요.<br> 
조금 극단적으로 데이터 샘플링이 되었을때 20% 뽑힌 Test 데이터가 전부 **비가 오는 날**이 되어버리면??? <br>
결국 비가 오지 **않는**날로 모델이 구해져 우리가 원하는 결과를 얻지 못하게 되겠습니다. <br><br>

이를 해결할수 있는 문제로 비가오는/오지않는 날을 구분지어 데이터를 샘플링 하게 되는 것입니다. 이것이 계층적 샘플링의 간단한 개념입니다.

다음은 중간 소득(median imcome)컬럼에 해당하는 데이터의 히스토그램입니다.


```python
housing['median_income'].hist();
```


![png](https://user-images.githubusercontent.com/45762604/83213812-0f311100-a19e-11ea-9385-61a458c8056e.png)


이 연속형 데이터를 조건에 맞추어서 범주형 데이터로 바꿔주도록 하겠습니다. 쉽게 말해서 소득에 등급을 메기려 합니다. 다음의 코드는 중간 소득을 1.5로 나눈 후에 정수로 바꿔주고 5보다 큰값은 모두 5로 바꾸어 주었습니다.


```python
housing['income_cat'] = np.ceil(housing['median_income'] / 1.5)
housing['income_cat'].where(housing['income_cat'] < 5, 5.0, inplace=True)
```


```python
housing["income_cat"].value_counts()
```




    3.0    7236
    2.0    6581
    4.0    3639
    5.0    2362
    1.0     822
    Name: income_cat, dtype: int64



결과적으로 다음처럼 1,2,3,4,5의 중간소득을 등급화 한 값을 볼 수 있습니다.


```python
housing["income_cat"].hist();
```


![png](https://user-images.githubusercontent.com/45762604/83213813-0fc9a780-a19e-11ea-80b3-47aa39322672.png)


이제 소득 카테고리를 기반으로 계층적 샘플링을 할 준비가 되었습니다. 사이킷런의 StratifiedShuffleSplit를 사용해주면 다음과 같습니다. 


```python
from sklearn.model_selection import StratifiedShuffleSplit

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=56)
for train_index, test_index in split.split(housing, housing["income_cat"]):
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index]
    
housing["income_cat"].value_counts() / len(housing)
```




    3.0    0.350581
    2.0    0.318847
    4.0    0.176308
    5.0    0.114438
    1.0    0.039826
    Name: income_cat, dtype: float64



### e) Random과 Stratified Sampling 비교 해보기

이렇게 1~5까지의 등급을 무작위 샘플링한 결과와, 계층 샘플링한 결과와 비교 해보도록 하겠습니다.


```python
def income_cat_proportions(data):
    return data["income_cat"].value_counts() / len(data)

train_set, test_set = train_test_split(housing, test_size=0.2, random_state=56)

compare_props = pd.DataFrame({
    "Overall": income_cat_proportions(housing),
    "Stratified": income_cat_proportions(strat_test_set),
    "Random": income_cat_proportions(test_set),
}).sort_index()
compare_props["Rand. %error"] = 100 * compare_props["Random"] / compare_props["Overall"] - 100
compare_props["Strat. %error"] = 100 * compare_props["Stratified"] / compare_props["Overall"] - 100

compare_props
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Overall</th>
      <th>Stratified</th>
      <th>Random</th>
      <th>Rand. %error</th>
      <th>Strat. %error</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1.0</th>
      <td>0.039826</td>
      <td>0.039729</td>
      <td>0.043605</td>
      <td>9.489051</td>
      <td>-0.243309</td>
    </tr>
    <tr>
      <th>2.0</th>
      <td>0.318847</td>
      <td>0.318798</td>
      <td>0.323886</td>
      <td>1.580307</td>
      <td>-0.015195</td>
    </tr>
    <tr>
      <th>3.0</th>
      <td>0.350581</td>
      <td>0.350533</td>
      <td>0.356589</td>
      <td>1.713654</td>
      <td>-0.013820</td>
    </tr>
    <tr>
      <th>4.0</th>
      <td>0.176308</td>
      <td>0.176357</td>
      <td>0.163275</td>
      <td>-7.392141</td>
      <td>0.027480</td>
    </tr>
    <tr>
      <th>5.0</th>
      <td>0.114438</td>
      <td>0.114583</td>
      <td>0.112645</td>
      <td>-1.566469</td>
      <td>0.127011</td>
    </tr>
  </tbody>
</table>
</div>




```python
for set_ in (strat_train_set, strat_test_set):
    set_.drop("income_cat", axis=1, inplace=True)
```

## 2.4 데이터 탐색 & 시각화

이제 본격적으로 데이터 탐색과 시각화를 진행해보도록 하겠습니다. 탐색과 시각화는 트레이닝 데이터로만 진행을 하려고 합니다.


```python
housing = strat_train_set.copy()
```

### 2.4.1 지리적 데이터 시각화

데이터 중에 경도(longitude), 위도(latitude)가 있습니다. 이를 통해 데이터를 시각화 해보도록 하겠습니다.


```python
housing.plot(kind="scatter",x = "longitude",y = "latitude")
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7f8489e4cd68>




![png](https://user-images.githubusercontent.com/45762604/83213816-10623e00-a19e-11ea-903c-4411175458c3.png)


그러나 데이터가 상당히 많은 상태에서 시각화한 결과로는 원하는 결과를 얻을수 없습니다. 따라서 밀집된 지역은 좀더 진하게, 그리고 진한 척도가 얼마나 밀집이 되었는지를 알기 위해 alpha옵션을 사용해 주도록 하겠습니다. alpha()옵션은 점들의 투명도로 보시면 되겠습니다.


```python
housing.plot(kind = "scatter", x="longitude", y = "latitude", alpha=0.1)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7f8489cfccf8>




![png](https://user-images.githubusercontent.com/45762604/83213818-10fad480-a19e-11ea-99c5-8ce276dc4f54.png)


조금만 더 나아가 봅시다. 인구가 밀집된 지역은 주택 가격이 높다라는 기본 지식을 생각해봅시다. 우리나라에서 서울을 떠올리시면 쉽게 이해가 가실텐데요. 그럼 캘리포니아는 어떨까요? 밀집된 지역에 주택 가격을 추가 해주고 이에 색상도 추가 해줘서 다음의 결과를 얻을 수 있습니다.


```python
housing.plot(kind = "scatter", x="longitude", y = "latitude", alpha=0.4,
            s=housing["population"]/100, label="population", figsize=(10,7), 
            c="median_house_value", cmap=plt.get_cmap("jet"), colorbar=True, 
            sharex=False)
plt.legend()
```




    <matplotlib.legend.Legend at 0x7f8489c64c50>




![png](https://user-images.githubusercontent.com/45762604/83213820-122c0180-a19e-11ea-8765-b8b4a0b1b8cf.png)


### 2.4.2 상관관계 조사

상관관계를 조사해보도록 하겠습니다. 표준상관계수(피어슨의 r이라고도 표현 하는)를 corr()메서드를 사용해 계산 해보도록 하겠습니다.


```python
corr_matrix = housing.corr()
```

이제 중간주택 가격과 다른 데이터들의 상관관계가 어떤지 보도록 하겠습니다. 조금 더 알아보기 쉽도록 sort_value메서드를 사용해 정렬을 시켰으며 ascending옵션에는 False를 하여 내림차순 옵션을 적용해주었습니다.


```python
corr_matrix["median_house_value"].sort_values(ascending=False)
```




    median_house_value    1.000000
    median_income         0.688608
    total_rooms           0.134397
    housing_median_age    0.098337
    households            0.065369
    total_bedrooms        0.049382
    population           -0.023751
    longitude            -0.042199
    latitude             -0.145617
    Name: median_house_value, dtype: float64



#### 상관 계수 시각화 

이번에는 상관 관계를 시각화해보도록 하겠습니다. 결과는 다음과 같습니다.


```python
from pandas.plotting import scatter_matrix

attributes = ["median_house_value", "median_income", "total_rooms", "housing_median_age"]

scatter_matrix(housing[attributes], figsize=(12,8))
```




    array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f8489db9860>,
            <matplotlib.axes._subplots.AxesSubplot object at 0x7f8489bbbc88>,
            <matplotlib.axes._subplots.AxesSubplot object at 0x7f8489b75278>,
            <matplotlib.axes._subplots.AxesSubplot object at 0x7f8489b24828>],
           [<matplotlib.axes._subplots.AxesSubplot object at 0x7f8489b56dd8>,
            <matplotlib.axes._subplots.AxesSubplot object at 0x7f8489b153c8>,
            <matplotlib.axes._subplots.AxesSubplot object at 0x7f8489ac5978>,
            <matplotlib.axes._subplots.AxesSubplot object at 0x7f8489a77f60>],
           [<matplotlib.axes._subplots.AxesSubplot object at 0x7f8489a77f98>,
            <matplotlib.axes._subplots.AxesSubplot object at 0x7f8489a63ac8>,
            <matplotlib.axes._subplots.AxesSubplot object at 0x7f8489a230b8>,
            <matplotlib.axes._subplots.AxesSubplot object at 0x7f8489ddae10>],
           [<matplotlib.axes._subplots.AxesSubplot object at 0x7f8489db2198>,
            <matplotlib.axes._subplots.AxesSubplot object at 0x7f8489ded2e8>,
            <matplotlib.axes._subplots.AxesSubplot object at 0x7f8489e5e438>,
            <matplotlib.axes._subplots.AxesSubplot object at 0x7f848a25da90>]],
          dtype=object)




![png](https://user-images.githubusercontent.com/45762604/83213821-12c49800-a19e-11ea-8299-0607a181ba66.png)


주택 가격과 소득이 가장 높은 상관 관계를 보이므로 좀더 크게 보기위해 다음과 같은 코드를 사용해 확인해보도록 하겠습니다. 크게보니 추가적으로 몇가지 사실을 더 알수 있습니다. 

- 상관관계 0.687160이 나온만큼 강합니다. 
- 500000과 약 350000되는 수치가 직선에 가까운 형태를 볼 수 있습니다. 이러한 데이터는 제거 하는것이 좋습니다. 


```python
housing.plot(kind="scatter", x="median_income",y="median_house_value",alpha=0.1)
```




    <matplotlib.axes._subplots.AxesSubplot at 0x7f8489f636a0>




![png](https://user-images.githubusercontent.com/45762604/83213822-135d2e80-a19e-11ea-80b1-3868b8887b7b.png)


### 2.4.3 특성 조합으로 실험

이번엔 데이터를 조금만 더 편집해서 상관관계를 보도록 하겠습니다.
- 가구당 방 개수 
- 방개수당 침실
- 가구당 인원

rooms_per_househole라고 새로 만들어준 가구당 방 개수와 주택 가격과 0.14의 상관관계를 갖는 수치로 추가 되었습니다. 


```python
housing["rooms_per_househole"] = housing["total_rooms"] / housing["households"]
housing["bedrooms_per_rooms"] = housing["total_bedrooms"] / housing["total_rooms"]
housing["populations_per_househole"] = housing["population"] / housing["households"]
```


```python
corr_matrix = housing.corr()
corr_matrix["median_house_value"].sort_values(ascending=False)
```




    median_house_value           1.000000
    median_income                0.688608
    rooms_per_househole          0.149027
    total_rooms                  0.134397
    housing_median_age           0.098337
    households                   0.065369
    total_bedrooms               0.049382
    populations_per_househole   -0.021812
    population                  -0.023751
    longitude                   -0.042199
    latitude                    -0.145617
    bedrooms_per_rooms          -0.254491
    Name: median_house_value, dtype: float64



## 2.5 머신러닝 알고리즘을 위한 데이터 준비

머신러닝 을 위해 데이터를 준비 해봅시다. 이때 수동으로 하는 방법대신 함수를 만들어 자동화 해야 하는 이유가 있습니다. 
- 어떤 데이터셋에 대해서도 데이터 변환을 손쉽게 반복할 수 있어야 합니다. 
- 향후 프로젝트에 유동적으로 사용이 가능해야 합니다. 


```python
housing = strat_train_set.drop("median_house_value", axis = 1)
housing_labels = strat_train_set["median_house_value"].copy()
```

### 2.5.1 데이터 정제

**데이터 전처리**라고도 합니다. 데이터를 분석하는데 있어서 상당히 많은 시간을 투자 해야 하는 작업중에 하나 입니다. 앞서 total_bedrooms에 결측치가 있는 경우가 있는데, 이를 해결할 수 있는 방법은 다음과 같습니다. 
- 해당 행을 제거 합니다. 
- 해당 컬럼을 제거 합니다.
- 특정 값으로 채웁니다. (0, 평균, 중간값 등)

이는 dropna(), drop(), fillna()메서드를 사용해 처리 할 수 있습니다. 


```python
housing.dropna(subset=["total_bedrooms"])
housing.drop("total_bedrooms", axis = 1)
median = housing["total_bedrooms"].median()
housing["total_bedrooms"].fillna(median, inplace=True)
```

#### sklearn Imputer

사이킷런의 SimpleInputer은 손쉽게 결측치를 다루게 해줍니다. 우선 결측치를 중간값으로 대체하는 코드를 사용해보도록 하겠습니다.범주형 데이터인 ocean_proximity는 중간값을 구할 수 없기에 복사본을 생성해주도록 합니다.


```python
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="median")

housing_num = housing.drop('ocean_proximity', axis=1)

imputer.fit(housing_num)
```




    SimpleImputer(add_indicator=False, copy=True, fill_value=None,
                  missing_values=nan, strategy='median', verbose=0)




```python
imputer.statistics_
```




    array([-118.5    ,   34.26   ,   29.     , 2122.5    ,  432.     ,
           1163.5    ,  408.     ,    3.54035])




```python
housing_num.median().values
```




    array([-118.5    ,   34.26   ,   29.     , 2122.5    ,  432.     ,
           1163.5    ,  408.     ,    3.54035])




```python
X = imputer.transform(housing_num)

housing_tr = pd.DataFrame(X, columns=housing_num.columns,
                          index=housing.index.values)
```

### 2.5.2 텍스트의 범주형 특성 다루기

이번에는 텍스트데이터인 ocean_proximity데이터를 다뤄보도록 하겠습니다. 중간값을 구할수 없으니 이를 구할수 있게 바꿔주도록 하겠습니다. 


```python
housing_cat = housing["ocean_proximity"]
housing_cat.head(10)
```




    19300     <1H OCEAN
    8590      <1H OCEAN
    2338         INLAND
    3729      <1H OCEAN
    19362    NEAR OCEAN
    11531     <1H OCEAN
    19304     <1H OCEAN
    8409      <1H OCEAN
    5100      <1H OCEAN
    6287      <1H OCEAN
    Name: ocean_proximity, dtype: object



중간값을 바꿔주는데에는 판다스의 factorize()메서드를 사용해주어 각 카테고리를 숫자로 바꿔줍니다. 늑대/고양이/강아지와 같은 데이터가 0/1/2와 같은 숫자로 바뀌는것과 같습니다.


```python
housing_cat_encoded, housing_categories = housing_cat.factorize()
housing_cat_encoded[:10]
```




    array([0, 0, 1, 0, 2, 0, 0, 0, 0, 0])




```python
housing_categories
```




    Index(['<1H OCEAN', 'INLAND', 'NEAR OCEAN', 'NEAR BAY', 'ISLAND'], dtype='object')



#### 원-핫 인코딩

이렇게 숫자로 바꾸어 주긴 했다만 앞서 예시로 두었던 늑대/고양이/강아지를 0/1/2로 바꾸어 주어 중간값을 구해버렸을때 다음과 같은 문제가 생깁니다. 
- 늑대와 강아지는 고양이와 비교 했을때 같은 부류로 볼 수 있는데 왜 0 - 2라고 먼 데이터가 되었을까?

이를 해결 할 수 있는것이 **원-핫 인코딩**입니다.원-핫 인코딩에 대한 자세한 설명은 구글 키워드로 대체 하도록 하겠습니다. 

- 구글 키워드: 원-핫 인코딩, one-hot encoding


```python
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder(categories='auto')
housing_cat_1hot = encoder.fit_transform(housing_cat_encoded.reshape(-1, 1))
housing_cat_1hot
```




    <16512x5 sparse matrix of type '<class 'numpy.float64'>'
    	with 16512 stored elements in Compressed Sparse Row format>




```python
housing_cat_1hot.toarray()
```




    array([[1., 0., 0., 0., 0.],
           [1., 0., 0., 0., 0.],
           [0., 1., 0., 0., 0.],
           ...,
           [0., 1., 0., 0., 0.],
           [1., 0., 0., 0., 0.],
           [1., 0., 0., 0., 0.]])




```python
# [PR #9151](https://github.com/scikit-learn/scikit-learn/pull/9151)에서 가져온 CategoricalEncoder 클래스의 정의.
# 이 클래스는 사이킷런 0.20에 포함될 예정입니다.

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils import check_array
from sklearn.preprocessing import LabelEncoder
from scipy import sparse

class CategoricalEncoder(BaseEstimator, TransformerMixin):
    """Encode categorical features as a numeric array.
    The input to this transformer should be a matrix of integers or strings,
    denoting the values taken on by categorical (discrete) features.
    The features can be encoded using a one-hot aka one-of-K scheme
    (``encoding='onehot'``, the default) or converted to ordinal integers
    (``encoding='ordinal'``).
    This encoding is needed for feeding categorical data to many scikit-learn
    estimators, notably linear models and SVMs with the standard kernels.
    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.
    Parameters
    ----------
    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'
        The type of encoding to use (default is 'onehot'):
        - 'onehot': encode the features using a one-hot aka one-of-K scheme
          (or also called 'dummy' encoding). This creates a binary column for
          each category and returns a sparse matrix.
        - 'onehot-dense': the same as 'onehot' but returns a dense array
          instead of a sparse matrix.
        - 'ordinal': encode the features as ordinal integers. This results in
          a single column of integers (0 to n_categories - 1) per feature.
    categories : 'auto' or a list of lists/arrays of values.
        Categories (unique values) per feature:
        - 'auto' : Determine categories automatically from the training data.
        - list : ``categories[i]`` holds the categories expected in the ith
          column. The passed categories are sorted before encoding the data
          (used categories can be found in the ``categories_`` attribute).
    dtype : number type, default np.float64
        Desired dtype of output.
    handle_unknown : 'error' (default) or 'ignore'
        Whether to raise an error or ignore if a unknown categorical feature is
        present during transform (default is to raise). When this is parameter
        is set to 'ignore' and an unknown category is encountered during
        transform, the resulting one-hot encoded columns for this feature
        will be all zeros.
        Ignoring unknown categories is not supported for
        ``encoding='ordinal'``.
    Attributes
    ----------
    categories_ : list of arrays
        The categories of each feature determined during fitting. When
        categories were specified manually, this holds the sorted categories
        (in order corresponding with output of `transform`).
    Examples
    --------
    Given a dataset with three features and two samples, we let the encoder
    find the maximum value per feature and transform the data to a binary
    one-hot encoding.
    >>> from sklearn.preprocessing import CategoricalEncoder
    >>> enc = CategoricalEncoder(handle_unknown='ignore')
    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])
    ... # doctest: +ELLIPSIS
    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,
              encoding='onehot', handle_unknown='ignore')
    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()
    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],
           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])
    See also
    --------
    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of
      integer ordinal features. The ``OneHotEncoder assumes`` that input
      features take on values in the range ``[0, max(feature)]`` instead of
      using the unique values.
    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of
      dictionary items (also handles string-valued features).
    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot
      encoding of dictionary items or strings.
    """

    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,
                 handle_unknown='error'):
        self.encoding = encoding
        self.categories = categories
        self.dtype = dtype
        self.handle_unknown = handle_unknown

    def fit(self, X, y=None):
        """Fit the CategoricalEncoder to X.
        Parameters
        ----------
        X : array-like, shape [n_samples, n_feature]
            The data to determine the categories of each feature.
        Returns
        -------
        self
        """

        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:
            template = ("encoding should be either 'onehot', 'onehot-dense' "
                        "or 'ordinal', got %s")
            raise ValueError(template % self.handle_unknown)

        if self.handle_unknown not in ['error', 'ignore']:
            template = ("handle_unknown should be either 'error' or "
                        "'ignore', got %s")
            raise ValueError(template % self.handle_unknown)

        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':
            raise ValueError("handle_unknown='ignore' is not supported for"
                             " encoding='ordinal'")

        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)
        n_samples, n_features = X.shape

        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]

        for i in range(n_features):
            le = self._label_encoders_[i]
            Xi = X[:, i]
            if self.categories == 'auto':
                le.fit(Xi)
            else:
                valid_mask = np.in1d(Xi, self.categories[i])
                if not np.all(valid_mask):
                    if self.handle_unknown == 'error':
                        diff = np.unique(Xi[~valid_mask])
                        msg = ("Found unknown categories {0} in column {1}"
                               " during fit".format(diff, i))
                        raise ValueError(msg)
                le.classes_ = np.array(np.sort(self.categories[i]))

        self.categories_ = [le.classes_ for le in self._label_encoders_]

        return self

    def transform(self, X):
        """Transform X using one-hot encoding.
        Parameters
        ----------
        X : array-like, shape [n_samples, n_features]
            The data to encode.
        Returns
        -------
        X_out : sparse matrix or a 2-d array
            Transformed input.
        """
        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)
        n_samples, n_features = X.shape
        X_int = np.zeros_like(X, dtype=np.int)
        X_mask = np.ones_like(X, dtype=np.bool)

        for i in range(n_features):
            valid_mask = np.in1d(X[:, i], self.categories_[i])

            if not np.all(valid_mask):
                if self.handle_unknown == 'error':
                    diff = np.unique(X[~valid_mask, i])
                    msg = ("Found unknown categories {0} in column {1}"
                           " during transform".format(diff, i))
                    raise ValueError(msg)
                else:
                    # Set the problematic rows to an acceptable value and
                    # continue `The rows are marked `X_mask` and will be
                    # removed later.
                    X_mask[:, i] = valid_mask
                    X[:, i][~valid_mask] = self.categories_[i][0]
            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])

        if self.encoding == 'ordinal':
            return X_int.astype(self.dtype, copy=False)

        mask = X_mask.ravel()
        n_values = [cats.shape[0] for cats in self.categories_]
        n_values = np.array([0] + n_values)
        indices = np.cumsum(n_values)

        column_indices = (X_int + indices[:-1]).ravel()[mask]
        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),
                                n_features)[mask]
        data = np.ones(n_samples * n_features)[mask]

        out = sparse.csc_matrix((data, (row_indices, column_indices)),
                                shape=(n_samples, indices[-1]),
                                dtype=self.dtype).tocsr()
        if self.encoding == 'onehot-dense':
            return out.toarray()
        else:
            return out
```


```python
cat_encoder = CategoricalEncoder()
housing_cat_reshaped = housing_cat.values.reshape(-1, 1)
housing_cat_1hot = cat_encoder.fit_transform(housing_cat_reshaped)
housing_cat_1hot
```




    <16512x5 sparse matrix of type '<class 'numpy.float64'>'
    	with 16512 stored elements in Compressed Sparse Row format>




```python
housing_cat_1hot.toarray()

```




    array([[1., 0., 0., 0., 0.],
           [1., 0., 0., 0., 0.],
           [0., 1., 0., 0., 0.],
           ...,
           [0., 1., 0., 0., 0.],
           [1., 0., 0., 0., 0.],
           [1., 0., 0., 0., 0.]])




```python
cat_encoder.categories_
```




    [array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],
           dtype=object)]



### 2.5.3 나만의 변환기

이렇게 숫자형, 범주형 데이터에 대한 전처리를 해주었으니 다음 작업을 해주도록 하겠습니다. 자세한 설명은 생략 하도록 하겠습니다.


```python

from sklearn.base import BaseEstimator, TransformerMixin

rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6

class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self, add_bedrooms_per_room=True):  
        self.add_bedrooms_per_room = add_bedrooms_per_room
        
    def fit(self, X, y=None):
        return self
    
    def transform(self, X, y=None):
        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]
        population_per_household = X[:, population_ix] / X[:, household_ix]
        if self.add_bedrooms_per_room:
            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
            return np.c_[X, rooms_per_household, population_per_household,
                         bedrooms_per_room]
        else:
            return np.c_[X, rooms_per_household, population_per_household]
        

attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)
housing_extra_attribs = attr_adder.transform(housing.values)
```


```python
housing_extra_attribs = pd.DataFrame(
    housing_extra_attribs, 
    columns=list(housing.columns)+["rooms_per_household", "population_per_household"])
housing_extra_attribs.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>ocean_proximity</th>
      <th>rooms_per_household</th>
      <th>population_per_household</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-122.83</td>
      <td>38.39</td>
      <td>19</td>
      <td>1765</td>
      <td>394</td>
      <td>868</td>
      <td>388</td>
      <td>2.462</td>
      <td>&lt;1H OCEAN</td>
      <td>4.54897</td>
      <td>2.23711</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-118.39</td>
      <td>33.89</td>
      <td>40</td>
      <td>826</td>
      <td>143</td>
      <td>389</td>
      <td>147</td>
      <td>7.1845</td>
      <td>&lt;1H OCEAN</td>
      <td>5.61905</td>
      <td>2.64626</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-119.68</td>
      <td>36.83</td>
      <td>11</td>
      <td>2455</td>
      <td>344</td>
      <td>1110</td>
      <td>339</td>
      <td>6.1133</td>
      <td>INLAND</td>
      <td>7.24189</td>
      <td>3.27434</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-118.42</td>
      <td>34.18</td>
      <td>40</td>
      <td>1013</td>
      <td>150</td>
      <td>449</td>
      <td>166</td>
      <td>5.7143</td>
      <td>&lt;1H OCEAN</td>
      <td>6.10241</td>
      <td>2.70482</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-123.49</td>
      <td>38.7</td>
      <td>9</td>
      <td>5409</td>
      <td>1019</td>
      <td>594</td>
      <td>327</td>
      <td>3.3125</td>
      <td>NEAR OCEAN</td>
      <td>16.5413</td>
      <td>1.81651</td>
    </tr>
  </tbody>
</table>
</div>



### 2.5.4 특성 스케일링

이번에는 스케일링입니다. 스케일링을 해주는 이유는 데이터들을 **공정하게** 판단 해주기 위함이라고 쉽게 이해를 해주면 좋겠는데요. 다음의 예시를 들어보도록 하겠습니다. <br> 
농구선수들의 키와 연봉은 어느정도 비례할까? 라는 질문에 대해 생각을 해보도록 하겠습니다. 농구선수들의 키는 180에서 210사이로 움직이는 반면 연봉은 3천만원에서 몇십,몇백억까지도 상승하는 경우가 있습니다. 이때 고작 180~210남짓한 수치와 몇십억에 해당하는 수치를 가지고 머신러닝을 작동시키면 잘 작동하지 않는 경우가 생깁니다.  <br>
이를 해결하기 위해 이 값들의 범위를 모두 같도록 해주는 방법입니다. min-max 스케일링과 표준화(standardization)이 가장 널리 사용되고 있습니다. 

- min-max 스케일링
    - 최소값을 0, 최대값을 1에 맞추어 그안에서 데이터들이 분포하게 해주는 방법을 말합니다. 
    - 사이킷런에서는 MinMaxScaler변환기를 제공합니다.
- 표준화(standardization)
    - 평균을 뺀후 표준편차로 나누어 분산이 1이 되도록 하는 방법입니다.
    - 공식은 다음과 같습니다. Z-Score라고 하며 고등학교때의 기억이 희미해져서인지 이를 정규화라고 잘못 알고 있는 분들이 많습니다.
$$
z = \frac{\text{data point} - \text{mean}}{\text{standard deviation}} = \frac{x - \mu}{\sigma}
$$
    - 사이킷런에서는 StandardScaler변환기를 제공합니다.



```python
from sklearn.preprocessing import MinMaxScaler

data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]
scaler = MinMaxScaler()
print(scaler.fit(data))
print("max of data = ", scaler.data_max_)
print("min of data = ", scaler.data_min_)
print("data range = ", scaler.data_range_)
print("<data scaling> \n", scaler.transform(data))
print('[2, 2] minmax scaling =', scaler.transform([[2, 2]]))
```

    MinMaxScaler(copy=True, feature_range=(0, 1))
    max of data =  [ 1. 18.]
    min of data =  [-1.  2.]
    data range =  [ 2. 16.]
    <data scaling> 
     [[0.   0.  ]
     [0.25 0.25]
     [0.5  0.5 ]
     [1.   1.  ]]
    [2, 2] minmax scaling = [[1.5 0. ]]



```python
from sklearn.preprocessing import StandardScaler

data = [[0, 0], [0, 0], [1, 1], [1, 1]]
scaler = StandardScaler()
print(scaler.fit(data))
print('mean = ', scaler.mean_)
print('variance = ', scaler.var_)
print('z-score = \n', scaler.transform(data))
print('[2, 2] transformation result = ', scaler.transform([[2, 2]]))
```

    StandardScaler(copy=True, with_mean=True, with_std=True)
    mean =  [0.5 0.5]
    variance =  [0.25 0.25]
    z-score = 
     [[-1. -1.]
     [-1. -1.]
     [ 1.  1.]
     [ 1.  1.]]
    [2, 2] transformation result =  [[3. 3.]]


### 2.5.5 변환 파이프라인

앞서 사용했던 단계의 순서를 정확하게 실행 해주는 코드 입니다.


```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

num_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy="median")),
        ('attribs_adder', CombinedAttributesAdder()),
        ('std_scaler', StandardScaler()),
    ])

housing_num_tr = num_pipeline.fit_transform(housing_num)
housing_num_tr
```




    array([[-1.62523661,  1.28975808, -0.76505408, ..., -0.34238863,
            -0.07472322,  0.14554974],
           [ 0.58977077, -0.81411382,  0.90432289, ...,  0.07297683,
            -0.03945122, -0.61942634],
           [-0.05377867,  0.56041582, -1.40100721, ...,  0.70290424,
             0.01469475, -1.12326236],
           ...,
           [-0.98168717,  1.34586133, -0.92404236, ...,  0.06794352,
            -0.07601963, -0.40061275],
           [ 0.5748045 , -0.73930949,  0.7453346 , ..., -0.08321968,
            -0.11135747,  0.25441383],
           [ 0.62469205, -0.80943855,  0.58634632, ..., -0.51051383,
            -0.01384681,  0.3503762 ]])




```python
from sklearn.base import BaseEstimator, TransformerMixin

class DataFrameSelector(BaseEstimator, TransformerMixin):
    def __init__(self, attribute_names):
        self.attribute_names = attribute_names
        
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        return X[self.attribute_names].values
```


```python
num_attribs = housing_num.columns.tolist()
cat_attribs = ['ocean_proximity']

num_pipeline = Pipeline([
    ('selector', DataFrameSelector(num_attribs)),
    ('imputer', SimpleImputer(strategy="median")),
    ('attribs_adder', CombinedAttributesAdder()),
    ('std_scaler', StandardScaler()),
])

cat_pipeline = Pipeline([
    ('selector', DataFrameSelector(cat_attribs)),
    ('cat_encoder', CategoricalEncoder(encoding="onehot-dense")),
])
```


```python
from sklearn.pipeline import FeatureUnion

full_pipeline = FeatureUnion(transformer_list=[
        ('num_pipeline', num_pipeline),
        ('cat_pipeline', cat_pipeline),
])
```


```python
housing_prepared = full_pipeline.fit_transform(housing)
housing_prepared
```




    array([[-1.62523661,  1.28975808, -0.76505408, ...,  0.        ,
             0.        ,  0.        ],
           [ 0.58977077, -0.81411382,  0.90432289, ...,  0.        ,
             0.        ,  0.        ],
           [-0.05377867,  0.56041582, -1.40100721, ...,  0.        ,
             0.        ,  0.        ],
           ...,
           [-0.98168717,  1.34586133, -0.92404236, ...,  0.        ,
             0.        ,  0.        ],
           [ 0.5748045 , -0.73930949,  0.7453346 , ...,  0.        ,
             0.        ,  0.        ],
           [ 0.62469205, -0.80943855,  0.58634632, ...,  0.        ,
             0.        ,  0.        ]])




```python
housing_prepared.shape
```




    (16512, 16)



## 2.6 모델 선택과 훈련

### 2.6.1 훈련 세트에서 훈련하고 평가받기

이제 모델을 훈련시켜 보도록 하겠습니다. 먼저 선형회귀 모델부터 해보도록 하겠습니다.

#### a) LinearRegression

간단합니다. 사이킷런에서 제공하는 LinearRegression을 사용해 다음과 같은 결과를 얻을 수 있습니다.


```python
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)
```




    LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)



이 값에 대해 Training set에 적용시켜 어느정도 결과를 낼 수 있는지 확인해봅시다.


```python
some_data = housing.iloc[:5]
some_labels = housing_labels.iloc[:5]
some_data_prepared = full_pipeline.transform(some_data)

print("예측:", lin_reg.predict(some_data_prepared))
print("레이블:", list(some_labels))
```

    예측: [178362.28950409 363869.25668282 206041.52692904 296284.5380043
     253502.09680704]
    레이블: [260300.0, 438100.0, 120000.0, 382400.0, 295400.0]


이번엔 RMSE를 구해보도록 하겠습니다. 사이킷런의 mean_square_error을 사용해 MSE를 구한 후에 루트를 씌워 RMSE를 구합니다.<br>
주택 가격의 대부분은 120,000~256,000을 알면 68,628의 RMSE는 좋은 결과가 아님을 알 수 있습니다.


```python
from sklearn.metrics import mean_squared_error

housing_predictions = lin_reg.predict(housing_prepared)
lin_mse = mean_squared_error(housing_labels, housing_predictions)
lin_rmse = np.sqrt(lin_mse)
print('RMSE :', lin_rmse)
```

    RMSE : 67999.11859158528


#### b) DecisionTree

이번에는 Decision Tree(의사결정나무)를 훈련시켜보도록 하겠습니다. 코드는 다음과 같습니다.


```python
from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor()
tree_reg.fit(housing_prepared, housing_labels)
```




    DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,
                          max_leaf_nodes=None, min_impurity_decrease=0.0,
                          min_impurity_split=None, min_samples_leaf=1,
                          min_samples_split=2, min_weight_fraction_leaf=0.0,
                          presort=False, random_state=None, splitter='best')



RMSE가 0. 너무 결과가 좋습니다. 이러한 경우 과대적합이 된것으로 볼 수 있습니다. **운**좋게도 잘 뽑힐 수 있는 데이터만 훈련이 되었을 지도 모르겠내요. 이러한 문제를 해결할 방법중에 하나가 **교차검증**입니다.


```python
housing_predictions = tree_reg.predict(housing_prepared)
tree_mse = mean_squared_error(housing_labels, housing_predictions)
tree_rmse = np.sqrt(tree_mse)
print('RMSE :', tree_rmse)
```

    RMSE : 0.0


### 2.6.2 교차 검증을 사용한 평가 

구글 키워드로 **K-교차검증**을 후에 찾아보도록 합시다. Cross Validation의 한글명인 교차검증은 Training/Test을 불리하는데 있어, 여러번(K번) 적용하여 훈련을 시켜줍니다. K번의 Training/Test set이 분리되기 때문에 과대적합을 피할수 있는 좋은 방법입니다. cross_val_score()함수를 사용해 교차검증을 진행 하고, cv=10을 하여 그룹 10개의 Traning/Test 데이터를 분리해서 평가해봅시다

#### a) Decission Tree의 교차검증

의사결정 나무가 직전에 했던것만큼 좋아보이지 않습니다. 평균 70,000정도의 RMSE와 2,500의 편차를 얻을수 있는데, 이로써 교차검증을 통해 직전에 실행되었던 의사결정나무의 결과가 과대적합임을 확인할 수 있었습니다.


```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(tree_reg, housing_prepared, housing_labels,
                         scoring='neg_mean_squared_error', cv=10)
tree_rmse_scores = np.sqrt(-scores)
```


```python
def display_scores(scores):
    print("Scores:", scores)
    print("Mean:", scores.mean())
    print("Std:", scores.std())

display_scores(tree_rmse_scores)
```

    Scores: [69068.62794545 70465.22147883 68918.17567685 69504.60723269
     70532.07133135 66233.17336462 73863.8781477  74439.73104391
     73184.42966101 66924.19657537]
    Mean: 70313.41124577759
    Std: 2650.1204605955795


#### b) Linear regression의 교차검증

이번에는 선형회귀 모델의 교차검증 결과를 보도록 하겠습니다. Decission Tree의 결과보다 좋습니다.추가로 모델을 하나 더 보도록 해보겠습니다. 다음에 사용해볼 모델은 랜덤포레스트 입니다.


```python
lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,
                             scoring="neg_mean_squared_error", cv=10)
lin_rmse_scores = np.sqrt(-lin_scores)
display_scores(lin_rmse_scores)
```

    Scores: [68203.67701574 69205.18213388 69259.0849722  67383.44961375
     68090.48067019 66396.94540104 72002.64166503 69116.88502908
     68071.81621838 65243.93778932]
    Mean: 68297.41005086077
    Std: 1732.6981851777248


#### c) RandomForest

랜덤포레스트는 특성을 무작위로 선택해서 많은 결정 트리를 만들고 그 예측의 평균으로 결과를 냅니다. 앙상블 학습이라고도 불리며 여러 다른 모델을 모아서 하나의 모델을 만들기 때문에 꽤 높은 성능을 보이는 방법입니다.


```python
from sklearn.ensemble import RandomForestRegressor

forest_reg = RandomForestRegressor(random_state=56)
forest_reg.fit(housing_prepared, housing_labels)
```

    /root/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.
      "10 in version 0.20 to 100 in 0.22.", FutureWarning)





    RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,
                          max_features='auto', max_leaf_nodes=None,
                          min_impurity_decrease=0.0, min_impurity_split=None,
                          min_samples_leaf=1, min_samples_split=2,
                          min_weight_fraction_leaf=0.0, n_estimators=10,
                          n_jobs=None, oob_score=False, random_state=56, verbose=0,
                          warm_start=False)


RMSE는 약 20,000으로 높아 보입니다. 이번엔 랜덤포레스트의 교차검증의 결과들 보도록 하겠습니다.

```python
housing_predictions = forest_reg.predict(housing_prepared)
forest_mse = mean_squared_error(housing_labels, housing_predictions)
forest_rmse = np.sqrt(forest_mse)
print('RMSE :', forest_rmse)
```

    RMSE : 22383.097464149505


랜덤 포레스트의 결과를 보니 평균 RMSE가 약 50,000, 표준편차가 약 2,000으로 이번에도 만족스럽지 못합니다. RMSE를 더 낮출수 있는 방법이 또 있을까요? 최대한 노력해보도록 하겠습니다. 따라서 다음에 해보려 하는것은 모델의 세부 튜닝입니다.


```python
from sklearn.model_selection import cross_val_score

forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,
                                scoring="neg_mean_squared_error", cv=10)
forest_rmse_scores = np.sqrt(-forest_scores)
```


```python
display_scores(forest_rmse_scores)
```

    Scores: [52555.15695956 53214.69056334 52194.56147758 52977.05962503
     50289.71740616 51783.06849353 52787.66108979 54156.46531064
     54345.71752626 49743.13845128]
    Mean: 52404.72369031759
    Std: 1413.3949433739967


## 모델 세부 튜닝

모델을 튜닝 해보도록 하겠습니다. 우선 그리드 탐색(보통은 Grid Search 라는 영어표현을 씁니다.)을 먼저 보도록 하겠습니다.

### 2.7.1 그리드 탐색

 랜덤포레스트라는 머신러닝 알고리즘을 돌리면 되는것이 전부가 아닙니다. 이 안에도 여러가지의 옵션값들이 있는데요. 이 값들을 조금씩 조정 해가면서 좋은 결과를 최대한을 내려고 합니다. 쉬운 이해를 돕기 위해 옵션값이라고 표현을 했지만, 하이퍼파라미터 조정이라는 표현을 같이 알아두면, 앞으로 협업하는데 또는 고객과의 의사소통에 지장은 없어 보입니다.<br>
 n_estimators, max_features가 각각 3, 4개로 총 12개의 파라미터 조정을 하고, 이후에 두번째 파라미터의 2x3 = 6개를 진행 합니다. 
총 18번의 모델이 돌아가는데요, 거기에 cv=5옵션으로 5겹 교차점증까지 진행하니 (12+6) x 5 = 90번의 모델을 돌리게 됩니다. 


```python
from sklearn.model_selection import GridSearchCV

param_grid = [
    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
  ]

forest_reg = RandomForestRegressor(random_state=42)

grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', 
                           return_train_score=True, n_jobs=-1)
grid_search.fit(housing_prepared, housing_labels)
```




    GridSearchCV(cv=5, error_score='raise-deprecating',
                 estimator=RandomForestRegressor(bootstrap=True, criterion='mse',
                                                 max_depth=None,
                                                 max_features='auto',
                                                 max_leaf_nodes=None,
                                                 min_impurity_decrease=0.0,
                                                 min_impurity_split=None,
                                                 min_samples_leaf=1,
                                                 min_samples_split=2,
                                                 min_weight_fraction_leaf=0.0,
                                                 n_estimators='warn', n_jobs=None,
                                                 oob_score=False, random_state=42,
                                                 verbose=0, warm_start=False),
                 iid='warn', n_jobs=-1,
                 param_grid=[{'max_features': [2, 4, 6, 8],
                              'n_estimators': [3, 10, 30]},
                             {'bootstrap': [False], 'max_features': [2, 3, 4],
                              'n_estimators': [3, 10]}],
                 pre_dispatch='2*n_jobs', refit=True, return_train_score=True,
                 scoring='neg_mean_squared_error', verbose=0)




```python
grid_search.best_params_
```




    {'max_features': 8, 'n_estimators': 30}



best_estimator메서드를 사용해 최적의 추정기에 직접 접근을 할 수도 있습니다.


```python
grid_search.best_estimator_
```




    RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,
                          max_features=8, max_leaf_nodes=None,
                          min_impurity_decrease=0.0, min_impurity_split=None,
                          min_samples_leaf=1, min_samples_split=2,
                          min_weight_fraction_leaf=0.0, n_estimators=30,
                          n_jobs=None, oob_score=False, random_state=42, verbose=0,
                          warm_start=False)



다음과 같이 각각의 평가 점수도 확인할 수 있습니다.


```python
cvres = grid_search.cv_results_
for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
    print(np.sqrt(-mean_score), params)
```

    64234.94802758031 {'max_features': 2, 'n_estimators': 3}
    55595.20942984079 {'max_features': 2, 'n_estimators': 10}
    52991.78408846735 {'max_features': 2, 'n_estimators': 30}
    60499.51302522474 {'max_features': 4, 'n_estimators': 3}
    52657.33361908652 {'max_features': 4, 'n_estimators': 10}
    50296.788419189565 {'max_features': 4, 'n_estimators': 30}
    58562.42100095029 {'max_features': 6, 'n_estimators': 3}
    51833.792932276585 {'max_features': 6, 'n_estimators': 10}
    49974.15728557978 {'max_features': 6, 'n_estimators': 30}
    58924.552193691045 {'max_features': 8, 'n_estimators': 3}
    52373.10304197352 {'max_features': 8, 'n_estimators': 10}
    49721.26272225661 {'max_features': 8, 'n_estimators': 30}
    61637.216903261724 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}
    53764.19907897887 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}
    59993.45186209557 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}
    52851.67683471628 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}
    57916.501024117315 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}
    51417.07292194993 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}


### 2.7.2 랜덤 탐색

Grid Search는 비교적 적은 수의 조합을 탐구할때 쓰이나, 많은것을 고려 하게 된다면 랜덤 탐색을 고려해야 할때가 있습니다. 랜덤 탐색의 장점은 다음과 같습니다. 
- 랜덤탐색을 1,000회로 했을 경우, 하이퍼파라미터마다 서로 다른 1,000개의 모델을 실행하게 됩니다.
- 반복횟수를 조절하느 ㄴ것만으로 하이퍼파라미터 탐색에 투입할 컴퓨팅 자원을 제어할 수 있습니다.


모델 튜닝에는 정말 **많은시간**이 걸립니다. 10분이 걸릴수도 때에 따라서는 몇시간이 걸릴수도 있습니다. 소소한 팁이 있다면 이런때에는 식사를 하거나, 빨래를 하거나, 방정리를 하는것이 좋아보입니다.


```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint

param_distribs = {
        'n_estimators': randint(low=1, high=200),
        'max_features': randint(low=1, high=8),
    }

forest_reg = RandomForestRegressor(random_state=56)
rnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,
                                n_iter=10, cv=5, scoring='neg_mean_squared_error', 
                                random_state=56, n_jobs=-1)
rnd_search.fit(housing_prepared, housing_labels)
```

    /root/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.
      "timeout or by a memory leak.", UserWarning





    RandomizedSearchCV(cv=5, error_score='raise-deprecating',
                       estimator=RandomForestRegressor(bootstrap=True,
                                                       criterion='mse',
                                                       max_depth=None,
                                                       max_features='auto',
                                                       max_leaf_nodes=None,
                                                       min_impurity_decrease=0.0,
                                                       min_impurity_split=None,
                                                       min_samples_leaf=1,
                                                       min_samples_split=2,
                                                       min_weight_fraction_leaf=0.0,
                                                       n_estimators='warn',
                                                       n_jobs=None, oob_score=False,
                                                       random_sta...
                                                       warm_start=False),
                       iid='warn', n_iter=10, n_jobs=-1,
                       param_distributions={'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f8488bc9748>,
                                            'n_estimators': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f8488bc9898>},
                       pre_dispatch='2*n_jobs', random_state=56, refit=True,
                       return_train_score=False, scoring='neg_mean_squared_error',
                       verbose=0)




```python

cvres = rnd_search.cv_results_
for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
    print(np.sqrt(-mean_score), params)
```

    49091.64158449113 {'max_features': 6, 'n_estimators': 144}
    53930.73445566795 {'max_features': 1, 'n_estimators': 163}
    49436.50795689067 {'max_features': 4, 'n_estimators': 123}
    49317.03975365767 {'max_features': 7, 'n_estimators': 88}
    49294.24987177641 {'max_features': 7, 'n_estimators': 101}
    51789.465921470626 {'max_features': 2, 'n_estimators': 91}
    51543.18675613576 {'max_features': 2, 'n_estimators': 153}
    49061.431052229214 {'max_features': 5, 'n_estimators': 195}
    49819.65557133584 {'max_features': 7, 'n_estimators': 44}
    49473.35657583965 {'max_features': 4, 'n_estimators': 111}


### 2.7.4 최상의 모델과 오차 분석

최상의 모델을 분석하면 문제에 대한 좋은 Insight를 얻는 경우가 많습니다. 각 특성에 대한 상대적인 중요도를 보는데 적합합니다.


```python
feature_importances = grid_search.best_estimator_.feature_importances_
feature_importances
```




    array([7.29908681e-02, 6.41615050e-02, 4.07619058e-02, 1.57706204e-02,
           1.45982002e-02, 1.46213116e-02, 1.39764138e-02, 3.64440721e-01,
           6.27158120e-02, 1.08890755e-01, 4.95425859e-02, 6.30048725e-03,
           1.66007098e-01, 1.20036299e-04, 1.91947703e-03, 3.18220334e-03])



다음은 중요도와 그에 대응하는 특성의 이름을 나타내었습니다.


```python
extra_attribs = ["rooms_per_hhold", "pop_per_hhold", "bedrooms_per_room"]
cat_encoder = cat_pipeline.named_steps["cat_encoder"]
cat_one_hot_attribs = list(cat_encoder.categories_[0])
attributes = num_attribs + extra_attribs + cat_one_hot_attribs
sorted(zip(feature_importances, attributes), reverse=True)
```




    [(0.36444072059219057, 'median_income'),
     (0.16600709752496975, 'INLAND'),
     (0.10889075510307952, 'pop_per_hhold'),
     (0.07299086814067803, 'longitude'),
     (0.06416150504052066, 'latitude'),
     (0.06271581204327017, 'rooms_per_hhold'),
     (0.04954258586760913, 'bedrooms_per_room'),
     (0.04076190579812007, 'housing_median_age'),
     (0.015770620407008204, 'total_rooms'),
     (0.014621311550431044, 'population'),
     (0.014598200214002776, 'total_bedrooms'),
     (0.013976413794862334, 'households'),
     (0.0063004872502870875, '<1H OCEAN'),
     (0.0031822033437083727, 'NEAR OCEAN'),
     (0.001919477029976488, 'NEAR BAY'),
     (0.00012003629928591163, 'ISLAND')]



### 2.7.5 테스트 세트로 시스템 평가 하기

만족할 만큼의 튜닝을 끝내면 이제서야 Test set에 평가를 진행 하도록 합니다. 다음의 결과를 통해 최종 RMSE를 확인할 수 있습니다. 이제는 이 결과를 가지고 비즈니스에 돌입할 준비를 해주면 되겠습니다. <br>


```python
final_model = grid_search.best_estimator_

X_test = strat_test_set.drop("median_house_value", axis=1)
y_test = strat_test_set["median_house_value"].copy()

X_test_prepared = full_pipeline.transform(X_test)
final_predictions = final_model.predict(X_test_prepared)

final_mse = mean_squared_error(y_test, final_predictions)
final_rmse = np.sqrt(final_mse)
final_rmse
```




    49091.5056690567

